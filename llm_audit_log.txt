2025-08-02 15:18:27,571 - oniks.llm.client - INFO - [LLM-REQUEST-6ae193f8] Starting LLM call at 2025-08-02T15:18:27.571764
2025-08-02 15:18:27,572 - oniks.llm.client - INFO - [LLM-REQUEST-6ae193f8] Model: llama3:8b
2025-08-02 15:18:27,572 - oniks.llm.client - INFO - [LLM-REQUEST-6ae193f8] Host: http://localhost:11434
2025-08-02 15:18:27,572 - oniks.llm.client - INFO - [LLM-REQUEST-6ae193f8] Prompt length: 100 characters
2025-08-02 15:18:27,572 - oniks.llm.client - INFO - [LLM-REQUEST-6ae193f8] FULL PROMPT BEGINS:
2025-08-02 15:18:27,572 - oniks.llm.client - INFO - [LLM-REQUEST-6ae193f8] Select the appropriate tool for this task: Create a file named 'test.txt' with content 'Hello World'
2025-08-02 15:18:27,572 - oniks.llm.client - INFO - [LLM-REQUEST-6ae193f8] FULL PROMPT ENDS
2025-08-02 15:18:27,584 - oniks.llm.client - ERROR - [LLM-ERROR-6ae193f8] Unexpected error at 2025-08-02T15:18:27.584242
2025-08-02 15:18:27,584 - oniks.llm.client - ERROR - [LLM-ERROR-6ae193f8] Error type: ConnectionError
2025-08-02 15:18:27,584 - oniks.llm.client - ERROR - [LLM-ERROR-6ae193f8] Error message: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download
2025-08-02 15:18:27,584 - oniks.llm.client - ERROR - [LLM-ERROR-6ae193f8] FULL TRACEBACK BEGINS:
2025-08-02 15:18:27,589 - oniks.llm.client - ERROR - [LLM-ERROR-6ae193f8] Traceback (most recent call last):
  File "/Users/danylohorlov/GitHub/ONIKS_NeuralNet/oniks/llm/client.py", line 97, in invoke
    response = self._client.chat(
        model=model,
    ...<9 lines>...
        }
    )
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ollama/_client.py", line 342, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ollama/_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ollama/_client.py", line 126, in _request_raw
    raise ConnectionError(CONNECTION_ERROR_MESSAGE) from None
ConnectionError: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download

2025-08-02 15:18:27,589 - oniks.llm.client - ERROR - [LLM-ERROR-6ae193f8] FULL TRACEBACK ENDS
2025-08-02 15:18:27,590 - oniks.agents.planner_agent - INFO - [PLANNER-f691b7cf] Starting LLM-powered decomposition at 2025-08-02T15:18:27.590029
2025-08-02 15:18:27,590 - oniks.agents.planner_agent - INFO - [PLANNER-f691b7cf] Goal to decompose: Create a file called 'demo.txt' with content 'Testing transparency' and then read it back
2025-08-02 15:18:27,590 - oniks.agents.planner_agent - INFO - [PLANNER-f691b7cf] Available tools: ['read_file', 'write_file', 'task_complete']
2025-08-02 15:18:27,590 - oniks.agents.planner_agent - INFO - [PLANNER-f691b7cf] Calling LLM for task decomposition
2025-08-02 15:18:27,590 - oniks.agents.planner_agent - ERROR - [PLANNER-f691b7cf] LLM decomposition failed at 2025-08-02T15:18:27.590496
2025-08-02 15:18:27,590 - oniks.agents.planner_agent - ERROR - [PLANNER-f691b7cf] Error type: OllamaConnectionError
2025-08-02 15:18:27,590 - oniks.agents.planner_agent - ERROR - [PLANNER-f691b7cf] Error message: Mock LLM failure for testing
2025-08-02 15:18:27,590 - oniks.agents.planner_agent - ERROR - [PLANNER-f691b7cf] FULL ERROR TRACEBACK BEGINS:
2025-08-02 15:18:27,591 - oniks.agents.planner_agent - ERROR - [PLANNER-f691b7cf] Traceback (most recent call last):
  File "/Users/danylohorlov/GitHub/ONIKS_NeuralNet/oniks/agents/planner_agent.py", line 138, in execute
    raw_llm_response = self.llm_client.invoke(decomposition_prompt)
  File "/Users/danylohorlov/GitHub/ONIKS_NeuralNet/test_llm_transparency.py", line 83, in invoke
    raise OllamaConnectionError("Mock LLM failure for testing")
oniks.llm.client.OllamaConnectionError: Mock LLM failure for testing

2025-08-02 15:18:27,591 - oniks.agents.planner_agent - ERROR - [PLANNER-f691b7cf] FULL ERROR TRACEBACK ENDS
2025-08-02 15:18:27,592 - oniks.agents.planner_agent - WARNING - [PLANNER-f691b7cf] Switching to fallback reasoning
2025-08-02 15:18:27,592 - oniks.agents.planner_agent - WARNING - [FALLBACK-75800bb2] USING HARDCODED FALLBACK REASONING - NO LLM INVOLVED
2025-08-02 15:18:27,592 - oniks.agents.planner_agent - WARNING - [FALLBACK-75800bb2] Goal to decompose: Create a file called 'demo.txt' with content 'Testing transparency' and then read it back
2025-08-02 15:18:27,592 - oniks.agents.planner_agent - WARNING - [FALLBACK-75800bb2] Generated simple file creation plan: ["write_file(file_path='example.txt', content='Example content')", 'task_complete()']
2025-08-02 15:18:27,592 - oniks.agents.planner_agent - INFO - [PLANNER-f691b7cf] Fallback plan with 2 steps:
2025-08-02 15:18:27,592 - oniks.agents.planner_agent - INFO - [PLANNER-f691b7cf] Fallback Step 1: write_file(file_path='example.txt', content='Example content')
2025-08-02 15:18:27,592 - oniks.agents.planner_agent - INFO - [PLANNER-f691b7cf] Fallback Step 2: task_complete()
2025-08-02 15:18:27,648 - oniks.llm.client - INFO - [LLM-REQUEST-4c9cd948] Starting LLM call at 2025-08-02T15:18:27.648340
2025-08-02 15:18:27,648 - oniks.llm.client - INFO - [LLM-REQUEST-4c9cd948] Model: llama3:8b
2025-08-02 15:18:27,648 - oniks.llm.client - INFO - [LLM-REQUEST-4c9cd948] Host: http://localhost:11434
2025-08-02 15:18:27,648 - oniks.llm.client - INFO - [LLM-REQUEST-4c9cd948] Prompt length: 30 characters
2025-08-02 15:18:27,648 - oniks.llm.client - INFO - [LLM-REQUEST-4c9cd948] FULL PROMPT BEGINS:
2025-08-02 15:18:27,648 - oniks.llm.client - INFO - [LLM-REQUEST-4c9cd948] What is the capital of France?
2025-08-02 15:18:27,648 - oniks.llm.client - INFO - [LLM-REQUEST-4c9cd948] FULL PROMPT ENDS
2025-08-02 15:18:27,650 - oniks.llm.client - ERROR - [LLM-ERROR-4c9cd948] Unexpected error at 2025-08-02T15:18:27.650325
2025-08-02 15:18:27,650 - oniks.llm.client - ERROR - [LLM-ERROR-4c9cd948] Error type: ConnectionError
2025-08-02 15:18:27,650 - oniks.llm.client - ERROR - [LLM-ERROR-4c9cd948] Error message: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download
2025-08-02 15:18:27,650 - oniks.llm.client - ERROR - [LLM-ERROR-4c9cd948] FULL TRACEBACK BEGINS:
2025-08-02 15:18:27,651 - oniks.llm.client - ERROR - [LLM-ERROR-4c9cd948] Traceback (most recent call last):
  File "/Users/danylohorlov/GitHub/ONIKS_NeuralNet/oniks/llm/client.py", line 97, in invoke
    response = self._client.chat(
        model=model,
    ...<9 lines>...
        }
    )
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ollama/_client.py", line 342, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ollama/_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ollama/_client.py", line 126, in _request_raw
    raise ConnectionError(CONNECTION_ERROR_MESSAGE) from None
ConnectionError: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download

2025-08-02 15:18:27,652 - oniks.llm.client - ERROR - [LLM-ERROR-4c9cd948] FULL TRACEBACK ENDS
2025-08-02 15:18:27,652 - oniks.llm.client - INFO - [LLM-REQUEST-d377ab1a] Starting LLM call at 2025-08-02T15:18:27.652165
2025-08-02 15:18:27,652 - oniks.llm.client - INFO - [LLM-REQUEST-d377ab1a] Model: llama3:8b
2025-08-02 15:18:27,652 - oniks.llm.client - INFO - [LLM-REQUEST-d377ab1a] Host: http://localhost:11434
2025-08-02 15:18:27,652 - oniks.llm.client - INFO - [LLM-REQUEST-d377ab1a] Prompt length: 35 characters
2025-08-02 15:18:27,652 - oniks.llm.client - INFO - [LLM-REQUEST-d377ab1a] FULL PROMPT BEGINS:
2025-08-02 15:18:27,652 - oniks.llm.client - INFO - [LLM-REQUEST-d377ab1a] How do you create a file in Python?
2025-08-02 15:18:27,652 - oniks.llm.client - INFO - [LLM-REQUEST-d377ab1a] FULL PROMPT ENDS
2025-08-02 15:18:27,654 - oniks.llm.client - ERROR - [LLM-ERROR-d377ab1a] Unexpected error at 2025-08-02T15:18:27.654019
2025-08-02 15:18:27,654 - oniks.llm.client - ERROR - [LLM-ERROR-d377ab1a] Error type: ConnectionError
2025-08-02 15:18:27,654 - oniks.llm.client - ERROR - [LLM-ERROR-d377ab1a] Error message: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download
2025-08-02 15:18:27,654 - oniks.llm.client - ERROR - [LLM-ERROR-d377ab1a] FULL TRACEBACK BEGINS:
2025-08-02 15:18:27,655 - oniks.llm.client - ERROR - [LLM-ERROR-d377ab1a] Traceback (most recent call last):
  File "/Users/danylohorlov/GitHub/ONIKS_NeuralNet/oniks/llm/client.py", line 97, in invoke
    response = self._client.chat(
        model=model,
    ...<9 lines>...
        }
    )
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ollama/_client.py", line 342, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ollama/_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ollama/_client.py", line 126, in _request_raw
    raise ConnectionError(CONNECTION_ERROR_MESSAGE) from None
ConnectionError: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download

2025-08-02 15:18:27,655 - oniks.llm.client - ERROR - [LLM-ERROR-d377ab1a] FULL TRACEBACK ENDS
2025-08-02 15:18:27,655 - oniks.llm.client - INFO - [LLM-REQUEST-5b175248] Starting LLM call at 2025-08-02T15:18:27.655739
2025-08-02 15:18:27,655 - oniks.llm.client - INFO - [LLM-REQUEST-5b175248] Model: llama3:8b
2025-08-02 15:18:27,655 - oniks.llm.client - INFO - [LLM-REQUEST-5b175248] Host: http://localhost:11434
2025-08-02 15:18:27,655 - oniks.llm.client - INFO - [LLM-REQUEST-5b175248] Prompt length: 27 characters
2025-08-02 15:18:27,656 - oniks.llm.client - INFO - [LLM-REQUEST-5b175248] FULL PROMPT BEGINS:
2025-08-02 15:18:27,656 - oniks.llm.client - INFO - [LLM-REQUEST-5b175248] Explain what a REST API is.
2025-08-02 15:18:27,656 - oniks.llm.client - INFO - [LLM-REQUEST-5b175248] FULL PROMPT ENDS
2025-08-02 15:18:27,657 - oniks.llm.client - ERROR - [LLM-ERROR-5b175248] Unexpected error at 2025-08-02T15:18:27.657518
2025-08-02 15:18:27,657 - oniks.llm.client - ERROR - [LLM-ERROR-5b175248] Error type: ConnectionError
2025-08-02 15:18:27,657 - oniks.llm.client - ERROR - [LLM-ERROR-5b175248] Error message: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download
2025-08-02 15:18:27,657 - oniks.llm.client - ERROR - [LLM-ERROR-5b175248] FULL TRACEBACK BEGINS:
2025-08-02 15:18:27,658 - oniks.llm.client - ERROR - [LLM-ERROR-5b175248] Traceback (most recent call last):
  File "/Users/danylohorlov/GitHub/ONIKS_NeuralNet/oniks/llm/client.py", line 97, in invoke
    response = self._client.chat(
        model=model,
    ...<9 lines>...
        }
    )
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ollama/_client.py", line 342, in chat
    return self._request(
           ~~~~~~~~~~~~~^
      ChatResponse,
      ^^^^^^^^^^^^^
    ...<12 lines>...
      stream=stream,
      ^^^^^^^^^^^^^^
    )
    ^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ollama/_client.py", line 180, in _request
    return cls(**self._request_raw(*args, **kwargs).json())
                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ollama/_client.py", line 126, in _request_raw
    raise ConnectionError(CONNECTION_ERROR_MESSAGE) from None
ConnectionError: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download

2025-08-02 15:18:27,659 - oniks.llm.client - ERROR - [LLM-ERROR-5b175248] FULL TRACEBACK ENDS
